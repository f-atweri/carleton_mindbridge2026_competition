# Environment Variables Template
# Copy this file to .env and fill in your actual values

# Ollama Configuration
# ====================
# Configure these variables to connect to your LLM backend

# Option 1: Carleton University LLM Server (recommended)
# Uncomment and fill in the URL provided by Research Computing Services
# OLLAMA_HOST=https://your-carleton-llm-server-url

# Option 2: Local Ollama (default)
# If not set, defaults to local Ollama at http://localhost:11434
# OLLAMA_HOST=http://localhost:11434

# Model Selection
# ===============
# Specify which model to use (must be available on your chosen server)
#
# Carleton Server models: llama3.3, llama3.2, mistral-large, mixtral, qwen3, gemma3, etc.
# Local models: depends on what you've pulled with `ollama pull <model>`
#
# Default is llama3.2 if not specified
OLLAMA_MODEL=llama3.2
